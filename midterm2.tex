\documentclass[10pt,letterpaper]{article}
\usepackage[letterpaper,margin=0.5cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{pdfpages}
\usepackage{amssymb}
\usepackage{siunitx}
\author{Jeffrey Wubbenhorst}
\title{Math 216 Midterm 2 Study Guide }

\begin{document}
\maketitle

\section*{Dimension, Linear Indepence} 
\subsection*{Subspaces and Spanning Sets }% chap 2.2, p 93 of bray notes
\begin{itemize}

\item Say $\alpha=\{v_1,...v_n\}$ is a basis for $V$, and $\vec{v}\in V$. Coordinates of $\vec{v}$ relative to the basis $\alpha$ are $[\vec{v}]_\alpha=\left(
\begin{array}{c} 
c_1 \\ \vdots \\ c_n
\end{array} \right)
$
where $c$ is the set of contstants for $\vec{v}=c_1\vec{v_1}+...+c_n\vec{v_n}$


\item A subset $W$ of a vector space $BV$ is a subspace of $V$ if $W$ is a subspace under addition, scalar multiplication of $V$ restricted to  $W$. (That is, if $W$ is closed under the same rules of scalar mulitplication and addition as $V$)

\item Let $W$ be a nonempty subset of a vector space $V$. $W$ is a subspace of $V$ iff, $\forall u, v \in W$ and $\forall c \in \mathbb{R}, u + w \in W, cu \in W$
% do I need to translate this into English tho 

\item There are some other rules for vector spaces: 
\begin{enumerate}
\item The zero vector is unique 
\item the negative of a vector $v\in V$ is unique 
\item $0\cdot \vec{v}=0$
\item $c\cdot \vec{0}=\vec{0}$
\item $(-1)\vec{v}=-\vec{v}$ 
\end{enumerate}
	

\item If $A$ is $m\times n$, solutions to system of homogenous linear equations $AX=0$ is a subspace of $\mathbb{R}^n$

\item A set of vectors $\{v_1, v_2, ..., v_n$ is \textbf{linearly independent }iff, for a system $c_1v_1+c_2v_2+...+c_nv_n=0$ all constants $c$ are zero. As a random note, no set containing the zero vector is independent. 

\end{itemize}	

\subsection*{Dimension, Nullspace, Row Space, Column Space }
% chapt 2.4 
\begin{itemize}

\item \textbf{Equivalent facts about nonsingular $n\times n$ matrices} (in particular some matrix $A$): \\
$A$ is nonsingular $\leftrightarrow$  
rref($A)=I \leftrightarrow rank(A) = n \leftrightarrow \\
A \mbox{ has existence property } \leftrightarrow
A \mbox{ has uniqueness property } \leftrightarrow \\
A \mbox{ is invertible } \leftrightarrow
A \mbox{ is a product of elementary matrices } \leftrightarrow \\
A \mbox{ represents a row reduction } \leftrightarrow
det(A) \neq 0 \leftrightarrow \\
\mbox{ columns of }A\mbox{ are linearly independent} \leftrightarrow
\mbox{ rows of }A\mbox{ are linearly independent} \leftrightarrow \\
dim(CS(A))=n \leftrightarrow
dim(RS(A)) = n \leftrightarrow 
dim(NS(A)) = 0
$


\item If a vector space $V$ has a basis of $n$ vectors, the \textbf{dimension of $V$ is $n$}. This is denoted as $\mbox{dim}(V)$. 

\item $\mbox{dim}(\mathbb{R}^n)=n, \mbox{dim}M_m\times n(\mathbb{R})=mn, \mbox{dim}(P_n)=n+1$
\item The \textbf{basis vectors of a polynomial} of degree $n$ are $\{x^n,x^{n-1},...,x,1\}$. This fundamental set of solutions \textbf{spans} $P_n$. 


\item Suppose some $v_1, v_2,...v_n$ in a vector space $V$. The vectors are \textbf{linearly dependent} iff \textit{only one} if $v_1,v_2,...,v_n$ is a linear combination of the others. To see if a set of vectors is linearly independent, solve for the constants for the homogenous equation; if a non-trivial solution exists, then the vectors are linearly dependent. 

\item Vectors $v_1,v_2,...,v_n $ of a vector space $V$ are a basis for $V$ if both of the following conditions are satisfied: 
\begin{enumerate}
\item $v_1,v_2,...,v_n$ are \textbf{linearly independent}
\item $v_1, v_2,...,v_n$ \textbf{span} $V$
\end{enumerate}

\item Suppose some $v_1, v_2,...v_n$ in a vector space $V$. Then $v_1, v_2,...,v_n$ form a basis for $V$ iff each vector in $V$ is uniquely expressible as a linear combonation of $v_1,v_2,...,v_n$

\item If $V$ is a bvector space adn $v_1, v_2, ...,v_n$ are vectors in $V$, then the set of all linear combinationfs of $v_1, v_2,...,v_n$ is a subspace of $V$ 

\item The subspace of some $V$ consisting of all linear combinations of vectors $v_1, v_2,...,v_n$ is referred to as the \textbf{subspace} of $V$ spanned by $v_1, v_2,...,v_n$ \\
...in English, the span is the set of all the different places you could ``go" if you combined the given vectors in every possible way. To see if something spans something else, solve for constants $c$; if there's no solution, then the set does not span. 

\item Suppose that $V$ is a vector space of dimension $n$. 
\begin{enumerate}
\item If the vecrtors $v_1,v_2,...,v_n$ are linearly independet, then $v_1,v_2, ...,v_n$ are linearly independent, then $v_1,v_2, ...,v_n$  form a basis for $V$

\item If $v_1,v_2, ...,v_n$ span $V$, then $v_1,v_2, ...,v_n$ form a \textbf{basis}
\end{enumerate}

\item The solutions to the homogenous system $AX=0$, where $A$ is an $m\times n$ matrix form a subspace of $\mathbb{R}^n$. This vector space of solutions is the \textbf{nullspace} or \textbf{kernel}$\mbox{ of } A, \mbox{ denoted by } NS(A)$

\item The \textbf{row space} of some matrix $A$ is the span of the rows of $A$. If $A, B$ are row-equivalent matrices, $RS(A)=RS(B)$

\item Weirdly enough, for an $m\times n \mbox{ matrix },\mbox{dim}(RS(A))+\mbox{dim}(NSA(A))=n$

\item The \textbf{column space} of a matrix $A$ is the subspace of $\mathbb{R}^m$ spanned by the columns of $A$ and is denoted by $CS(A)$. To find the column space of a matrix, you: 
\begin{enumerate}
\item \textbf{T}ranspose 
\item \textbf{R}ow-reduce
\item transpose back 
\item take \textbf{B}asis vectors 
\end{enumerate}
...dumb mnemonic is \textbf{TR}i\textbf{B}e. (Or `tribble' if you're into Star Trek.) 

\item For a given matrix $A, \mbox{dim}(RS(A)) = \mbox{dim}(CS(A))$; this common dimension is called the \textbf{rank} of $A$. 

\end{itemize}

\subsection*{Wronskians} % chapter 2.5 

\begin{itemize}
% * internally debates whether or not to actually bother representing what a wronskian is in LaTeX * 
\item Suppose some $f_1, f_2,...,f_n$ are functions in $D^{n-1}(a,b).\mbox{ If the \textbf{Wronskian} }w(f_1(x),f_2(x),...,f_n(x)) \mbox{ of } f_1,f_2,...,f_n$
is nonzero for some $x$ in $(a,b)$, then $f_1, f_2, ...,f_n \mbox{ are linearly independent elements of }D^{n-1}(a,b)$

\item Let $y_1,...y_n$ be solutions to the $n$th-order homogenous DE (satisfying the uniqueness/existence THM)
$L(y)=0$, then 
\\ $(w(x)=0 \mbox{ for any }x_0\rightarrow (\{y_1,...,y_n\}\mbox{ is linearly dependent }$
\item \textbf{Note:} The Wonskian can be computed for any convenient value of $x$. 
Try to find values that simplify the computations. 

\item \textit{Note:} It \textit{cannot} be said that $w(x)=0\Rightarrow\{f_1,...,f_n\}$ are linearly dependent!

\item A function $f: \mathbb{R}' \rightarrow \mathbb{R}'$ is \textbf{analytic} (also ``real analytic") if its Taylor series converges to itself. For our purposes, this means that all sums, products, and fractions of $e^x, \cos x, \sin x$, and all polynomials will are analytic functions. 

\item If $f_1,...,f_n$ are \textbf{analytic }and the \textbf{Wronskian }is identically zero (that is, zero everywhere), then  $\{f_1,...,f_n\}$ is \textbf{linearly dependent}. 

\iffalse 
\item If $\{y_1,...,y_n\}$ are solutions to the $n$th order linear homogenous DE $L(y)=0$, 
then: 
\begin{enumerate}
\item The Wronskian is always 0, functions are linearly dependent 
\item The Wronskian is never 0, functions are linearly independent 
\end{enumerate} % what is the point of this 
\fi 
\end{itemize}


% 4.3 
\section*{Differential Equations}

\subsection*{General}
\begin{itemize}

\item The \textbf{general solution} gives us all possible solutions to a given differential equation

\item \textbf{Existence and uniqueness theorem:}If $a,b>0$ and $f$ and $\partial f/\partial y$ are continuous on the rectangle $|x-x_0|<a$, 
and $|y-y_0|<b$, then there exists an $h>0$ so that the IVP 
$$y'=f(x,y), y(x_0)=y_0$$
has one \textit{and only one} solution for $|x-x_0|\leq h$

\item \textbf{Rational root theorem}: If $f(x)=a_nx^n+...+a_1x+a_0$ has \textit{integer} coefficients zand if $r=p/q$ is a \textit{rational} root, then $p$ divides $a_0$ and $a$ divides $a_n$. SO, a rational root $p/q\mbox{ of }\lambda^3-5\lambda^2+6\lambda-2=0$ must have $p$ dividing 2 and $q$ dividing 1, giving $\pm 1\mbox{ and } \pm 2$ as the only possible rational roots. 

\end{itemize}

\subsection*{Modeling with Diff Eq}

\begin{itemize}
\item \textbf{Continuously compounded interest, population, radioactive decay:} 
$t=Ce^{kt}$, where $C>0$ and $k$ are constants. \textbf{Exponential growth} if $k>0$, \textbf{exponential decay} if $k<0$

\item \textbf{Temperature} changes are determined by the equation 
$\theta = k(\theta_{environment} - \theta_{initial}$

\end{itemize}

\subsection*{Higher-order LDE}

\begin{itemize}
\item If $p(\lambda)$ has all real roots, giving solutions of the forms $e^{r_ix}\mbox{ and }x^ke^{r_ix}$
then the solutions are \textbf{linearly independent}.

\item For solutions to a differential equation $y_1,...,y_n$ on an interval $(a,b)$, if $w(y_1(x_0),...,y_n(x_0))=0$ for any $x_0$ in $(a,b)$, then $y_1,...,y_n$ are linearly dependent on $(a,b)$. \textit{Note that this \textbf{only} holds if the number of functions is equal to the order of the differential equation.}

\item If $r_1,...,r_k$ are distinct real roots for a characteristic polynomial of form 
$$a_ny^{(n)}+a_{n-1}y^{(n-1)}+...+a_1y'+a_0y=0$$
then $e^{r_1},...,e^{r_kx}$ are linearly independent solutions to this DE, and form a fundamenmtal set of solutions. 

\item 

\end{itemize}


\subsection*{Complex Solutions}
\begin{itemize}

\item \textit{\textbf{Note: }there is an entire section on complex numbers at the end!}

\item if $L(y)=0$ is a CCLDE w/ real coefficientsr and if $y(x) = \mu(x) + iv(x)$ is a complex-valued solution, $\mu(x), v(x)$ are also solutions. 

\item If $L(y)=0$ is a real CCLDE and $r, \bar{r}$ are a pair of roots of characteristic polynomial, then 
$e^{ax}\cos bx, e^{ax}s\sin bx$ are real, independent solutions, with same span as $e^{rx}, e^{\bar{r}x}$

\item If $L(Y)=0$ has a \textbf{characteristic polynomial} with roots $r_1, ...,r_n$, and sets of solutions formed by: 
\begin{enumerate}
\item For real roots $r$ of multiplicity $m$, we include 
$$e^{rx}, xe^{rx}, ..., x^{m-1}e^{rx}$$
\item For complex roots $r$ of multiplicity $m$, we include
 $$e^{rx}\cos bx, e^{rx}\sin bx, xe^{rx}\cos bx, xe^{rx}\sin bx , ..., x^{m-1}e^{rx}\cos bx, x^{m-1}e^{rx}\sin bx$$
\end{enumerate}
...then this set of functions is \textbf{independent}, and is a \textbf{fundamental set }of solutions. 


\end{itemize}



\subsection*{Method of Undetermined Coefficients}
\begin{itemize}
\item The solution to a differential equation is the sum of the particular solution and the homogenous solution: 
$$ y= y_h+y_p$$
\item The goal is to find some $y_p$ that works with some given differential equation, with $\lambda = r$ is a root of multiplicity $m$, $k$ is highest power of $x$ on the right side of the equation. 
\item If the root to an LDE of the form $a_ny^{(n)}+...+a_1y'+a_0y=Ax^ke^{rx}$ is real, the particular solution will be $y_p=x^m(A_kx^k+...+A_1x+A_0)e^{rx}$
\item If root to equation of form 
$$ a_ny^{(n)}+...+a_1y'+a_0y=Ax^ke^{ax}\cos bx+Bx^ke^{ax}\sin bx $$
is imaginary, particular solution is given as: 
$$ y_p = x^m(A_kx^k+...+A_1x+A_0)e^{ax}\cos bx + x^m(B_kx^k+...+B_1x+B_0)e^{ax}\sin bx $$
\item \textit{Note: the method of undetermined coefficients is \underline{not} an exact science!} There can be trial and error involved; a table of decent guesses is embedded at the end of this document. 
\end{itemize}

\subsection*{Applications} % chap 4.5 + errata 
\begin{itemize}
\item Sitautions with no external forces are usually given in the form 
$$F = mu'' + fu' + ku = 0$$
where (in spring problems at least) $k$ is the spring constant, $f$ is the friction coefficient, and mass $m$. (Don't forget that pounds are a unit of force, \textit{not} of mass!)

\item In \textbf{unforced cases} (no external input to system), the quadratic equation can give us roots; the issue of imaginary roots arises (that is, $f^2-4km<0$): 
\begin{enumerate}
\item If no friction ($f=0$), roots are imaginary. These cases are kind of rare. 
\item If $f-4km<0$, $\lambda= -a\pm bi$; in this case, the oscillating thing in question takes a bit to stop moving. This system is \textbf{underdamped}.  
\item If $f-4km>0$, there will be two roots $r_1, r_2 < 0$, solutions will have the form $u=c_1e^{r_1t}+c_2e^{r_2t}$
this system is \textbf{overdamped}. 
\item If $f^2-4km=0$, we have one root $r=\frac{-f}{2m}$ and solutions $u=c_1e^{rt}+c_2te^{rt}$. This system is \textbf{critically damped}, and is the case where the oscillating thing returns to rest within the shortest amount of time. 
\end{enumerate}

\item In \textbf{forced cases}, there are a few possible situations: 
\begin{enumerate}
\item For instances of no friction (system is \textbf{undamped}), the equation will have the form: 
$$\mu'' + ku = h(t)$$
We'll consider a case where $\omega_0\neq \omega$, wher $\omega = \sqrt{k/m}$; complete set of solutions can be obtained by: 
$$\mu_H = c_1\cos\omega_0 t + c_2\sin \omega_0 t$$
$$\mu_P = \frac{a}{\omega_0^2-\omega^2}\cos\omega t$$
...and $\mu = \mu_H+\mu_P$. 
\item \textbf{Resonance} occurs when $\omega = \omega_0$; in these situations, the amplitude increases as time increases. This has been known to cause some kinds of problems. 
% add gain and phase shift stuff in?? 
% add in some photos of what these different scenarios look like 

\end{enumerate}


\end{itemize}


% chapt 5.1 
\section*{Linear Transformations}
\begin{itemize}
\item A function $f$ from a set $X$ to a set $Y$ is denoted as $f : X \to Y$; $X$ is the domain of $f$, $Y$ is called the \textbf{image set }or \textbf{codomain}. The subset ${f(x)| x \in X}$ of Y is called the \textbf{range}, which, in English, means ``all the $f(x)$ that `hit' something in $Y$."

\item If $V$, 	$W$ are vector spaces, a function $T : V \to W$ is called a \textbf{linear transformation} if, for all vectors $u, v \in V$ and all scalars $c$, the following two properties hold: 
\begin{enumerate}
\item $T(u+ v) = T(u) +  T(v)$ 
\item $T(cv) = cT(v)$ 
\end{enumerate}

\item If $T : V \to V$, $T$ is sometimes called a \textbf{linear operator }

% theorem 5.1
\item If $A$ is an $m\times n$ matrix, $T : \mathbb{R}^n \to \mathbb{R}^m $ defined by 
$$T(X) = AX$$ is a linear transformation, but is more commonly called a \textbf{matrix transformation}

\item The differential operator takes derivatives, and is a linear transformation denoted by 
$$ D : D(a, b) \to F(a, b)$$

\item  $\mbox{Int}(f)$ denotes definite integral of a function $f$ over a closed interval $[a,b]$

% theorem 5.2
\item For some \textbf{linear transformation}$T : V \to W$ the following properties hold: 
\begin{enumerate}
\item $T(0)=0$
\item $T(-v)=0T(v) $ for any $v\in V$
\item $T(u-v)=T(u)-T(v)$ for any $u, v \in V$ 
\item $T(c_1v_1+c_2v_2+...+c_kv_k)=c_1T(v_1)+c_2T(v_2)+...+c_kT(v_k)$ for any scalars 
$c_1, c_2,...,c_k$ and any vectors $v_1, v_2,...v_k\in V$
\end{enumerate}

\item To determine what exactly a linear transformation does (assuming you have a set of input vectors by which the behavior of $T$ may be observed): 
\begin{enumerate} 
\item Assume, find, or steal some basis for the domain; this will determine all values of an LT. 
\item Determine the coefficients $c_1, ...,c_n$ for the basis vectors that yield a given output 
\item Determine how $T$ combines the given input vectors to an output vector of form  $[x_0,x_1,...,x_n]$, solving again for coefficients
\end{enumerate}

\item The \textbf{kernel} of $T$, denoted $\mbox{ker}(T)$, is defined as 
$$\mbox{ker}(T)=\{ v\in V | T(v)=0\}$$ 
...in English, the kernel is the set of all vectors that give an output of the zero vector. For matrix transformations, the kernel of the matrix transformation $T(X)=AX$ is the same as the nullspace of $A$: 
$$\mbox{ker}(T)=NS(A)$$ % huhuhuhu 

\item To find a basis for the column space of a matrix, take the transpose (\textbf{T}), reduce (\textbf{R}), transpose (\textbf{T}), split into columns (\textbf{S}), which approximately spells \textbf{ToRToiSe}.

\item Weirdly enough, if $T : V \to W$ is an LT where $V$ is a finite-dimensional vector space, we have that 
$$\mbox{dim(ker}(T)+\mbox{dim(range}(T)) = \mbox{dim}(V)$$

\end{itemize}

\section*{Errata} 

\begin{itemize}
\item The \textbf{contrapositive }of a statement is the reverse of the premise and conclusion, and the negation of both. Original statement: ``Duke won, and I'm happy." Contrapositive: ``If I'm not happy, then Duke didn't win."
\item \textbf{Quadratic equation:} $\frac{-b\pm \sqrt{b^2-4ac}}{2a}$ 
\end{itemize}

\subsection*{Trig Identities}
\begin{itemize}
\item STUFF HERE 
\item $\sin (\alpha +\beta )=\sin \alpha \cos \alpha + \sin \beta \cos \alpha$ 
\item $\sin (\alpha - \beta ) = \sin\alpha \cos \beta - \sin \beta \cos \alpha $ 
\item $\cos (\alpha + \beta ) = \cos \alpha \cos \beta - \sin \alpha \sin \beta $ 
\item $\cos (\alpha - \beta ) = \cos \alpha \cos \beta + \sin \alpha \sin \beta $
\end{itemize}

\subsection*{Complex stuff!! }
add it yo 
\begin{itemize}
\item $e^{i\theta}=\cos\theta+i\sin\theta$
\item $(r_1e^{i\theta_1})(r_2e^{i\theta_2})=(r_1r_2)e^{i(\theta_1\theta_2)}$
\item \textbf{UNDERSTAND COMPLEX CONJUGATES }
\item \textbf{Roots of unity:} Let $\alpha = e^{2\pi i/n}. \mbox{ Then, } 1, \alpha, \alpha^2,...,\alpha^{n-1}$ 
are the $n$th roots of 1. 
\item ...applying this to any complex number: Let $r$ be any such root. (That is, if
$\alpha = me^{i\theta}, r=(m^{1/n})e^i\theta/n$. In this case, the nth roots of 
$\alpha$ are \textbf{NOTE THIS ISN"T DONE!!! 1}
\item ...and, to that end, I am pirating someone else's work for this section. Thank you to RPI's Math 6800. 

\end{itemize}

\includepdf[pages={1}]{complexCheatSheet.pdf}
% this is pretty cool 
\includepdf[pages={1}]{trialSoln.pdf}


\iffalse 
#### sources 
* http://inside.mines.edu/fs_home/gmurray/teach/s02/trialSoln.pdf
* http://homepages.rpi.edu/~henshw/courses/fall2014/math6800/complexCheatSheet.pdf

\fi 

\end{document}

